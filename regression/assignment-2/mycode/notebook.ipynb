{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'v1'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import data_processing_v1 as data\n",
    "\n",
    "data.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting ... \n",
      "\n",
      "handling missing data ... \n",
      "\n",
      "preprocessing data... \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ngothai/opt/miniconda3/envs/tf/lib/python3.7/site-packages/feature_engine/encoding/one_hot.py:249: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[feature] == category, 1, 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing duplicate features ... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "houses_data = data.Data('../dataset/test.csv', '../dataset/train.csv')\n",
    "houses_data.run_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ngothai/opt/miniconda3/envs/tf/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  FutureWarning,\n",
      "/Users/ngothai/opt/miniconda3/envs/tf/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.837e+11, tolerance: 7.592e+08\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "/Users/ngothai/opt/miniconda3/envs/tf/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  FutureWarning,\n",
      "/Users/ngothai/opt/miniconda3/envs/tf/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.073e+11, tolerance: 7.288e+08\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "/Users/ngothai/opt/miniconda3/envs/tf/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  FutureWarning,\n",
      "/Users/ngothai/opt/miniconda3/envs/tf/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.043e+11, tolerance: 6.990e+08\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "/Users/ngothai/opt/miniconda3/envs/tf/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  FutureWarning,\n",
      "/Users/ngothai/opt/miniconda3/envs/tf/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.854e+11, tolerance: 7.706e+08\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "/Users/ngothai/opt/miniconda3/envs/tf/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  FutureWarning,\n",
      "/Users/ngothai/opt/miniconda3/envs/tf/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.565e+11, tolerance: 7.249e+08\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "/Users/ngothai/opt/miniconda3/envs/tf/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  FutureWarning,\n",
      "/Users/ngothai/opt/miniconda3/envs/tf/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.152e+09, tolerance: 7.592e+08\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "/Users/ngothai/opt/miniconda3/envs/tf/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  FutureWarning,\n",
      "/Users/ngothai/opt/miniconda3/envs/tf/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.717e+09, tolerance: 7.288e+08\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "/Users/ngothai/opt/miniconda3/envs/tf/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  FutureWarning,\n",
      "/Users/ngothai/opt/miniconda3/envs/tf/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.992e+09, tolerance: 6.990e+08\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "/Users/ngothai/opt/miniconda3/envs/tf/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  FutureWarning,\n",
      "/Users/ngothai/opt/miniconda3/envs/tf/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.348e+09, tolerance: 7.706e+08\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "/Users/ngothai/opt/miniconda3/envs/tf/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  FutureWarning,\n",
      "/Users/ngothai/opt/miniconda3/envs/tf/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.587e+09, tolerance: 7.249e+08\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "/Users/ngothai/opt/miniconda3/envs/tf/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  FutureWarning,\n",
      "/Users/ngothai/opt/miniconda3/envs/tf/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  FutureWarning,\n",
      "/Users/ngothai/opt/miniconda3/envs/tf/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  FutureWarning,\n",
      "/Users/ngothai/opt/miniconda3/envs/tf/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  FutureWarning,\n",
      "/Users/ngothai/opt/miniconda3/envs/tf/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  FutureWarning,\n",
      "/Users/ngothai/opt/miniconda3/envs/tf/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  FutureWarning,\n",
      "/Users/ngothai/opt/miniconda3/envs/tf/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  FutureWarning,\n",
      "/Users/ngothai/opt/miniconda3/envs/tf/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  FutureWarning,\n",
      "/Users/ngothai/opt/miniconda3/envs/tf/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  FutureWarning,\n",
      "/Users/ngothai/opt/miniconda3/envs/tf/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  FutureWarning,\n",
      "/Users/ngothai/opt/miniconda3/envs/tf/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  FutureWarning,\n",
      "/Users/ngothai/opt/miniconda3/envs/tf/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.948e+09, tolerance: 9.208e+08\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    }
   ],
   "source": [
    "train_lasso_selected, test_lasso_selected = houses_data.select_features_by_lasso()\n",
    "train_recursive_ellimination_selected, test_recursive_ellimination_selected = houses_data.select_features_by_recursive_feature_elimination()\n",
    "train_recursive_addition_selected, test_recursive_addition_selected = houses_data.select_features_by_recursive_feature_addition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.linear_model import Ridge, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Số fold bằng 10\n",
    "n_folds = 10\n",
    "y = houses_data.target_train\n",
    "\n",
    "def rmse_cv(model, train, y):\n",
    "  \n",
    "    # Tạo danh sách các fold\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=0).get_n_splits(train.values)\n",
    "\n",
    "    # Tiến hành kiểm chứng chéo với metric là MSE\n",
    "    mse= np.sqrt(-cross_val_score(model, train.values, np.log(y), scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "\n",
    "    # Trả về mảng giá trị MSE của từng fold\n",
    "    return(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Selection: Averaged base models score: 0.1676 (0.0338)\n",
      "\n",
      "Recursive Ellimination Selection: Averaged base models score: 0.1677 (0.0343)\n",
      "\n",
      "Recursive Addition Selection: Averaged base models score: 0.2058 (0.0279)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Ridge(alpha = 1e+3, tol = 0.0001, random_state=0)\n",
    "\n",
    "score = rmse_cv(model, train_lasso_selected, y)\n",
    "print(\"Lasso Selection: Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "\n",
    "score = rmse_cv(model, train_recursive_ellimination_selected, y)\n",
    "print(\"Recursive Ellimination Selection: Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "\n",
    "score = rmse_cv(model, train_recursive_addition_selected, y)\n",
    "print(\"Recursive Addition Selection: Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Selection: Averaged base models score: 0.3019 (0.0233)\n",
      "\n",
      "Recursive Ellimination Selection: Averaged base models score: 0.3042 (0.0235)\n",
      "\n",
      "Recursive Addition Selection: Averaged base models score: 0.3480 (0.0256)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = ElasticNet(alpha = 8, l1_ratio = 0.01, tol = 0.0001, random_state=0)\n",
    "\n",
    "## PUT YOUR CODE HERE:\n",
    "# Kiểm chứng chéo trên các tập dữ liệu đã được lựa chọn thuộc tính\n",
    "\n",
    "score = rmse_cv(model, train_lasso_selected, y)\n",
    "print(\"Lasso Selection: Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "\n",
    "score = rmse_cv(model, train_recursive_ellimination_selected, y)\n",
    "print(\"Recursive Ellimination Selection: Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "\n",
    "score = rmse_cv(model, train_recursive_addition_selected, y)\n",
    "print(\"Recursive Addition Selection: Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Selection: Averaged base models score: 4.8789 (0.6320)\n",
      "\n",
      "Recursive Ellimination Selection: Averaged base models score: 4.1538 (0.5859)\n",
      "\n",
      "Recursive Addition Selection: Averaged base models score: 25.1777 (3.0435)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = SVR(kernel = 'sigmoid')\n",
    "\n",
    "## PUT YOUR CODE HERE:\n",
    "# Kiểm chứng chéo trên các tập dữ liệu đã được lựa chọn thuộc tính\n",
    "\n",
    "score = rmse_cv(model, train_lasso_selected, y)\n",
    "print(\"Lasso Selection: Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "\n",
    "score = rmse_cv(model, train_recursive_ellimination_selected, y)\n",
    "print(\"Recursive Ellimination Selection: Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "\n",
    "score = rmse_cv(model, train_recursive_addition_selected, y)\n",
    "print(\"Recursive Addition Selection: Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Selection: Averaged base models score: 0.1959 (0.0247)\n",
      "\n",
      "Recursive Ellimination Selection: Averaged base models score: 0.2042 (0.0170)\n",
      "\n",
      "Recursive Addition Selection: Averaged base models score: 0.2132 (0.0227)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeRegressor(random_state = 0)\n",
    "\n",
    "## PUT YOUR CODE HERE:\n",
    "# Kiểm chứng chéo trên các tập dữ liệu đã được lựa chọn thuộc tính\n",
    "\n",
    "score = rmse_cv(model, train_lasso_selected, y)\n",
    "print(\"Lasso Selection: Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "\n",
    "score = rmse_cv(model, train_recursive_ellimination_selected, y)\n",
    "print(\"Recursive Ellimination Selection: Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "\n",
    "score = rmse_cv(model, train_recursive_addition_selected, y)\n",
    "print(\"Recursive Addition Selection: Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Selection: Averaged base models score: 0.1487 (0.0201)\n",
      "\n",
      "Recursive Ellimination Selection: Averaged base models score: 0.1492 (0.0213)\n",
      "\n",
      "Recursive Addition Selection: Averaged base models score: 0.1613 (0.0149)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
    "\n",
    "## PUT YOUR CODE HERE:\n",
    "# Kiểm chứng chéo trên các tập dữ liệu đã được lựa chọn thuộc tính\n",
    "\n",
    "score = rmse_cv(model, train_lasso_selected, y)\n",
    "print(\"Lasso Selection: Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "\n",
    "score = rmse_cv(model, train_recursive_ellimination_selected, y)\n",
    "print(\"Recursive Ellimination Selection: Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "\n",
    "score = rmse_cv(model, train_recursive_addition_selected, y)\n",
    "print(\"Recursive Addition Selection: Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tìm kiếm tham số tối ưu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=RandomForestRegressor(random_state=0),\n",
       "             param_grid={'n_estimators': [10, 20, 50, 100]},\n",
       "             scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'n_estimators': [10, 20, 50, 100]}\n",
    "\n",
    "# Khởi tạo lưới tìm kiếm trên mô hình đã được lựa chọn\n",
    "# trên tập dữ liệu loại đặc trưng bằng Lasso\n",
    "model_lasso = RandomForestRegressor(random_state = 0)\n",
    "grid_lasso = GridSearchCV(model_lasso, parameters , scoring = 'neg_mean_squared_error')\n",
    "grid_lasso.fit(train_lasso_selected, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 100}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_lasso.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=RandomForestRegressor(random_state=0),\n",
       "             param_grid={'n_estimators': [10, 20, 50, 100]},\n",
       "             scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'n_estimators': [10, 20, 50, 100]}\n",
    "\n",
    "# Khởi tạo lưới tìm kiếm trên mô hình đã được lựa chọn\n",
    "# trên tập dữ liệu loại đặc trưng bằng đệ quy\n",
    "model_recursive_ellimination = RandomForestRegressor(random_state = 0)\n",
    "grid_recursive_ellimination = GridSearchCV(model_recursive_ellimination, parameters, scoring = 'neg_mean_squared_error')\n",
    "grid_recursive_ellimination.fit(train_recursive_ellimination_selected, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 100}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_recursive_ellimination.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=RandomForestRegressor(random_state=0),\n",
       "             param_grid={'n_estimators': [10, 20, 50, 100]},\n",
       "             scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'n_estimators': [10, 20, 50, 100]}\n",
    "\n",
    "# Khởi tạo lưới tìm kiếm trên mô hình đã được lựa chọn \n",
    "# trên tập dữ liệu thêm đặc trưng bằng đệ quy\n",
    "model_recursive_addition = RandomForestRegressor(random_state = 0)\n",
    "grid_recursive_addition = GridSearchCV(model_recursive_addition, parameters, scoring = 'neg_mean_squared_error')\n",
    "grid_recursive_addition.fit(train_recursive_addition_selected, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 100}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_recursive_addition.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Selection: Averaged base models score: 0.1418 (0.0182)\n",
      "\n",
      "Recursive Ellimination Selection: Averaged base models score: 0.1424 (0.0185)\n",
      "\n",
      "Recursive Addition Selection: Averaged base models score: 0.1557 (0.0137)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_lasso = RandomForestRegressor(n_estimators = grid_lasso.best_params_['n_estimators'], random_state = 0)\n",
    "score = rmse_cv(model_lasso, train_lasso_selected, y)\n",
    "print(\"Lasso Selection: Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "\n",
    "model_recursive = RandomForestRegressor(n_estimators = grid_recursive_ellimination.best_params_['n_estimators'], random_state = 0)\n",
    "score = rmse_cv(model_recursive, train_recursive_ellimination_selected, y)\n",
    "print(\"Recursive Ellimination Selection: Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "\n",
    "model_recursive = RandomForestRegressor(n_estimators = grid_recursive_addition.best_params_['n_estimators'], random_state = 0)\n",
    "score = rmse_cv(model_recursive, train_recursive_addition_selected, y)\n",
    "print(\"Recursive Addition Selection: Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([123069.33, 159403.  , 187687.75, ..., 161290.77, 120084.5 ,\n",
       "       236484.18])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lasso = RandomForestRegressor(n_estimators = grid_lasso.best_params_['n_estimators'], random_state = 0)\n",
    "model_lasso.fit(train_lasso_selected, y)\n",
    "model_lasso.predict(test_lasso_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([123112.41, 162812.5 , 190189.33, ..., 168185.43, 121541.25,\n",
       "       236862.25])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ellimination = RandomForestRegressor(n_estimators = grid_recursive_ellimination.best_params_['n_estimators'], random_state = 0)\n",
    "model_ellimination.fit(train_recursive_ellimination_selected, y)\n",
    "model_ellimination.predict(test_recursive_ellimination_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([119945.83, 157791.5 , 194978.2 , ..., 146535.6 , 123754.5 ,\n",
       "       232439.  ])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_addition = RandomForestRegressor(n_estimators = grid_recursive_addition.best_params_['n_estimators'], random_state = 0)\n",
    "model_addition.fit(train_recursive_addition_selected, y)\n",
    "model_addition.predict(test_recursive_addition_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo file dùng để submit\n",
    "sub = pd.DataFrame()\n",
    "sub['Id'] = houses_data.test_ids\n",
    "sub['SalePrice'] = model_lasso.predict(test_lasso_selected)\n",
    "filename = 'submission_lasso_select-'+data.__version__+'.csv'\n",
    "sub.to_csv(filename,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo file dùng để submit\n",
    "sub = pd.DataFrame()\n",
    "sub['Id'] = houses_data.test_ids\n",
    "sub['SalePrice'] = model_ellimination.predict(test_recursive_ellimination_selected)\n",
    "filename = 'submission_recuresive_ellimination-'+data.__version__+'.csv'\n",
    "sub.to_csv(filename,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo file dùng để submit\n",
    "sub = pd.DataFrame()\n",
    "sub['Id'] = houses_data.test_ids\n",
    "sub['SalePrice'] = model_addition.predict(test_recursive_addition_selected)\n",
    "filename = 'submission_recuresive_addition-'+data.__version__+'.csv'\n",
    "sub.to_csv(filename,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Điền các score sau khi submit lên kaggle vào ô trống:\n",
    "\n",
    "- Lasso Selection: `0.14626`\n",
    "- Recursive Addition Feature: `0.15422`\n",
    "- Recursive Ellimination Feature: `0.14628`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4769b5f4808649e9f6efc2edf9fbe2018aef5c0a679de8e6bc3eb348403e35b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
