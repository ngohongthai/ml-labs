{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.1 Công cụ Tiền xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import các thư viện \n",
    "\n",
    "*   **NumPy**: cho phép chúng ta làm việc với mảng vì bạn sẽ thấy các mô hình học máy trong tương lai sẽ cần một số mảng làm input.\n",
    "*   **Matplotlib**: cho phép chúng ta vẽ một số biểu đồ rất tuyệt. \n",
    "*   **Pandas**: không chỉ cho phép chúng ta import tập dữ liệu mà còn tạo ma trận các feature và vectơ biến phụ thuộc. Tất nhiên những điều này sẽ được giải thích sau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import tập dữ liệu\n",
    "\n",
    " Chúng ta sẽ tìm hiểu cách import tập dữ liệu sau: **Data.csv** là một tập dữ liệu rất đơn giản: giả sử một công ty bán lẻ đang thực hiện một số phân tích về những khách hàng đã mua một trong các sản phẩm của họ. Vì vậy, các hàng trong tập dữ liệu này tương ứng với các khách hàng khác nhau. Và đối với mỗi khách hàng này, chúng ta có quốc gia, tuổi tác, mức lương và liệu họ có mua sản phẩm hay không.\n",
    "\n",
    " Có một nguyên tắc quan trọng hàng đầu trong ML. Trong bất kỳ tập dữ liệu nào mà bạn huấn luyện mô hình học máy sẽ có các thực thể giống nhau là các vectơ **features** và **dependent variable (biến phụ thuộc)**.\n",
    "\n",
    "**features** là các cột mà bạn sẽ dự đoán biến phụ thuộc và **dependent variable** là cột cuối cùng bởi vì công ty này muốn dự đoán xem liệu một số khách hàng tương lai có mua một sản phẩm nhất định dựa trên thông tin này. Vì vậy, các feature hay còn gọi là các biến độc lập, là các biến chứa một số thông tin mà bạn có thể dự đoán những gì bạn muốn dự đoán được gọi là biến phụ thuộc.\n",
    "\n",
    "Bạn sẽ có các feature riêng biệt, thường ở các cột đầu tiên của tập tập dữ liệu và biến phụ thuộc thường ở cột cuối cùng của tập dữ liệu. Hãy tạo hai thực thể này, gọi chúng là X cho ma trận các feature và Y cho vectơ biến phụ thuộc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Age</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>44.0</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spain</td>\n",
       "      <td>27.0</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>30.0</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spain</td>\n",
       "      <td>38.0</td>\n",
       "      <td>61000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>France</td>\n",
       "      <td>35.0</td>\n",
       "      <td>58000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Spain</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>France</td>\n",
       "      <td>48.0</td>\n",
       "      <td>79000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Germany</td>\n",
       "      <td>50.0</td>\n",
       "      <td>83000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>France</td>\n",
       "      <td>37.0</td>\n",
       "      <td>67000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country   Age   Salary Purchased\n",
       "0   France  44.0  72000.0        No\n",
       "1    Spain  27.0  48000.0       Yes\n",
       "2  Germany  30.0  54000.0        No\n",
       "3    Spain  38.0  61000.0        No\n",
       "4  Germany  40.0      NaN       Yes\n",
       "5   France  35.0  58000.0       Yes\n",
       "6    Spain   NaN  52000.0        No\n",
       "7   France  48.0  79000.0       Yes\n",
       "8  Germany  50.0  83000.0        No\n",
       "9   France  37.0  67000.0       Yes"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('data/Data.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xử lý dữ liệu bị thiếu\n",
    "\n",
    "Nếu xem lại tập dữ liệu **Data.csv**, chúng ta thấy rằng ở đây còn thiếu mức lương cho khách hàng cụ thể 40 tuổi đến từ Đức và đã mua một sản phẩm. Nhìn chung, bạn không muốn có bất kỳ dữ liệu nào bị thiếu trong tập dữ liệu đơn giản là vì nó có thể gây ra một số lỗi khi huấn luyện mô hình, do đó bạn phải xử lý chúng. Thực tế có một số cách để xử lý:\n",
    "\n",
    "\n",
    "*   Bỏ qua quan sát bằng cách xóa nó.\n",
    "*   Thay thế giá trị bị thiếu bằng giá trị trung bình của tất cả các giá trị trong cột mà dữ liệu bị thiếu, **đây là những gì mà bây giờ chúng ta đang thêm trong bộ công cụ.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 44.0, 72000.0],\n",
       "       ['Spain', 27.0, 48000.0],\n",
       "       ['Germany', 30.0, 54000.0],\n",
       "       ['Spain', 38.0, 61000.0],\n",
       "       ['Germany', 40.0, 63777.77777777778],\n",
       "       ['France', 35.0, 58000.0],\n",
       "       ['Spain', 38.77777777777778, 52000.0],\n",
       "       ['France', 48.0, 79000.0],\n",
       "       ['Germany', 50.0, 83000.0],\n",
       "       ['France', 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer.fit(X[:, 1:3])\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mã hóa dữ liệu phân loại\n",
    "\n",
    "Như chúng ta có thể thấy, tập dữ liệu này chứa một cột với các category (hạng mục): Pháp, Tây Ban Nha hoặc Đức. Có thể đoán rằng sẽ rất khó để mô hình học máy tính toán một số mối tương quan giữa các cột này, các feature và outcome là biến phụ thuộc và do đó sẽ phải chuyển các string này, các hạng mục này thành số."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mã hóa biến độc lập (X)\n",
    "\n",
    "Sử dụng biểu diễn One-hot (One-hot Encoding) để mã hóa các biến độc lập."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0, 0.0, 0.0, 44.0, 72000.0],\n",
       "       [0.0, 0.0, 1.0, 27.0, 48000.0],\n",
       "       [0.0, 1.0, 0.0, 30.0, 54000.0],\n",
       "       [0.0, 0.0, 1.0, 38.0, 61000.0],\n",
       "       [0.0, 1.0, 0.0, 40.0, 63777.77777777778],\n",
       "       [1.0, 0.0, 0.0, 35.0, 58000.0],\n",
       "       [0.0, 0.0, 1.0, 38.77777777777778, 52000.0],\n",
       "       [1.0, 0.0, 0.0, 48.0, 79000.0],\n",
       "       [0.0, 1.0, 0.0, 50.0, 83000.0],\n",
       "       [1.0, 0.0, 0.0, 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# 0 is the column index,\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough') \n",
    "X = np.array(ct.fit_transform(X))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mã hóa biến phụ thuộc (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_y = LabelEncoder()\n",
    "y = labelencoder_y.fit_transform(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chia tập dữ liệu thành Training set và Test set\n",
    "\n",
    "Việc chia tập dữ liệu thành training set (tập huấn luyện) trong một test bao gồm việc tạo hai tập riêng biệt, một training set mà bạn sẽ huấn luyện mô hình máy về các quan sát hiện có và một test set (tập kiểm tra) để đánh giá hiệu suất của mô hình dựa trên các quan sát mới. Bạn sẽ sử dụng **train_test_split** từ *scikit-learning* để thực hiện việc tách với **test_size = 0.2** của tập dữ liệu gốc và **random_state = 1** để đảm bảo bạn sẽ có cùng tập dữ liệu khi thực hiện chia tách."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Feature scaling là một phương pháp được sử dụng để chuẩn hóa biên độ (range) của các biến độc lập hoặc các feature của dữ liệu.\n",
    "\n",
    "Tại sao chúng ta cần làm điều này? Đó là do với một số mô hình học máy, để tránh một số feature bị chi phối bởi các feature khác theo cách mà các feature bị chi phối thậm chí không được mô hình học máy xem xét. Cũng cần lưu ý rằng chúng ta **sẽ không phải áp dụng feature scaling cho tất cả các mô hình học máy** mà chỉ áp dụng cho một số mô hình.\n",
    "\n",
    "Bạn sẽ mở rộng tập dữ liệu của mình bằng cách sử dụng **StandardScaler** từ *scikit-learning*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0, 0.0, 0.0, -0.7529426005471072, -0.6260377781240918],\n",
       "       [1.0, 0.0, 0.0, 1.008453807952985, 1.0130429500553495],\n",
       "       [1.0, 0.0, 0.0, 1.7912966561752484, 1.8325833141450703],\n",
       "       [0.0, 1.0, 0.0, -1.7314961608249362, -1.0943465576039322],\n",
       "       [1.0, 0.0, 0.0, -0.3615211764359756, 0.42765697570554906],\n",
       "       [0.0, 1.0, 0.0, 0.22561095973072184, 0.05040823668012247],\n",
       "       [0.0, 0.0, 1.0, -0.16581046438040975, -0.27480619351421154],\n",
       "       [0.0, 0.0, 1.0, -0.013591021670525094, -1.3285009473438525]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\n",
    "X_test[:, 3:] = sc.transform(X_test[:, 3:])\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 1.0, 0.0, 2.1827180802863797, 2.3008920936249107],\n",
       "       [0.0, 0.0, 1.0, -2.3186282969916334, -1.7968097268236927]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liệu chúng ta có phải áp dụng feature scaling trước khi tách tập dữ liệu thành training set (tập huấn luyện) và kiểm tra lúc đó hay sau đó không? Một số người cho rằng chúng ta cần áp dụng feature scaling trước khi nó tách ra. Một số người cho rằng cần thực hiện sau khi tách. **Câu trả lời là chúng ta cần áp dụng feature scaling sau khi tách tập dữ liệu thành tập huấn luyện**.\n",
    "\n",
    "Hãy để tôi giải thích lý do tại sao chúng ta cần áp dụng feature scaling sau khi tách tập dữ liệu thành tập huấn luyện và kiểm tra nó thực sự rõ ràng. Đơn giản là vì test set (tập kiểm tra) được cho là một tập hợp hoàn toàn mới mà bạn sẽ đánh giá mô hình học máy. Vì vậy, nó khá giống như bạn đang huấn luyện mô hình học máy trên tập huấn luyện. Sau khi nó được huấn luyện, bạn sẽ triển khai nó trên các quan sát mới. Điều này có nghĩa là tập kiểm tra là thứ mà bạn không nên làm cho huấn luyện và các feature. Scaling là một kỹ thuật sẽ lấy mean và độ lệch chuẩn của feature để thực hiện chia tỷ lệ. Vì vậy, nếu chúng ta áp dụng feature scaling trước khi phân tách thì nó sẽ thực sự nhận được mean và độ lệch chuẩn của tất cả các giá trị, gồm các giá trị trong tập kiểm tra. Và vì test là thứ bạn không cần phải có, giống như một số dữ liệu tương lai trong quá trình sản xuất. Việc áp dụng các feature trên tập dữ liệu gốc trước khi phân tách sẽ gây ra **rò rỉ thông tin** trên tập kiểm tra. Chúng ta sẽ lấy một số thông tin từ tập kiểm tra mà chúng ta không nên lấy vì nó được cho là dữ liệu mới với các quan sát mới.\n",
    "\n",
    "Vì vậy, hãy nhớ điều này. Lý do quan trọng mà bạn không nên áp dụng feature scaling trước khi phân tách là để ngăn chặn rò rỉ thông tin trên tập kiểm tra, điều mà bạn không nên có cho đến khi thực hiện xong quá trình huấn luyện."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c1bcb329fc666a29bd647ab5f731eda18fd18511409ff8d016e109c58957a965"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
